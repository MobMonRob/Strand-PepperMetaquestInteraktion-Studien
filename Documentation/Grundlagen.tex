\chapter{Grundlagen}
\label{chap:Grundlagen}

\section{Pepper}
\label{sec:Pepper}
\par Pepper ist ein humanoider Roboter, der entwickelt wurde, um die Gefühle und Gesten von Menschen zu analysieren und basierend auf diesen, darauf zu reagieren. Das Projekt entstand durch eine Zusammenarbeit des französischen Unternehmens Aldebaran Robotics \ac{SAS} und des japanischen Telekommunikations- und Medienkonzerns SoftBank Mobile Corp. Ziel diese Projektes war es, einen humanuiden ``Roboter-Gefährten'' oder einen ``persönlichen Roboter-Freund'' zu schaffen, der zunächst im Gewerbesektor in Verkaufsräumen, an Empfangstischen oder in Bildungs- und Gesundheitseinrichtungen eingesetzt werden sollte. Die Produktion wurde jedoch aufgrund geringer Nachfrage bis auf Weiteres pausiert.
\par Das Konzept von Pepper distanziert sich von herkömmlichen Industrierobotern und reinen Spielzeugrobotern, indem er als informativer und kommunikativer Begleiter konzipiert wurde. Sein Aussehen, das im etwa an die Größe eines Kindes angelehnt ist, sowie ein freundliches Gesicht und eine kindliche Stimme sind im ästhetischen Konzept von ``kawaii'' (japanisch für ``niedlich'' oder auch ``liebenswert'') gehalten.
\par Pepper wurde im Rahmen einer Präsentation am 5. Juni 2014 als der ``erste persönliche Roboter der Welt mit Emotionen'' vorgestellt. Die Vermarktung begann damit, dass SoftBank Pepper-Geräte in ihren Verkaufsräumen einsetzte, um Kunden zu unterhalten und zu informieren. Die Roboter sollte dabei den Umgang mit Kunden erlernen, um zukünftige Anwendungsmöglichkeiten zu erforschen. Verkauft wurde offiziell ab dem 3. Juli 2015 zu einem Preis von 198.000 Yen pro Einheit, zuzüglich monatlicher Gebühren für Zusatzleistungen. Im Laufe der Zeit wurde Pepper auch für den Einsatz in weiteren Unternehmen und Einrichtungen verfügbar gemacht.
\par Pepper wird mit einer Grundausstattung an Anwendungen geliefert, jedoch sind für spezifische Anwendungen, individuell entwickelte Softwarelösungen erforderlich wie auch zum Beispiel in diesem. SoftBank ermöglichte unabhängigen Entwicklern durch die Veröffentlichung der Schnittstellen den Zugang zu einem Interface für Applikationsprogramme, um zusätzliche Anwendungen für Pepper zu erstellen. Das NAOqi-Framework welches für diesen Nutzen bereitgestellt wurde, beinhaltet eine \ac{API}, eine \ac{SDK} und weitere Tools, welche in den Sprachen Python und C++ uneingeschränkten Zugriff auf die Komponenten, Sensoren und Aktoren des Roboters bieten, dazu später Ausführlicheres in \autoref{sec:NAOqi}. Mit Hilfe diese Interfaces haben verschiedene Unternehmen integrierte Lösungen entwickelt, die Pepper beispielsweise bei der Kundenberatung unterstützen können.
\par Das Design von Pepper ist dem Menschen ähnlich und umfasst einen Kopf mit integrierten Mikrofonen und Kameras sowie einen Torso mit weiteren Sensoren für Stabilität und Sicherheit. Der Roboter verfügt über verschiedene Mechaniken, die es ihm ermöglichen, sich flüssig zu bewegen und mit Personen zu interagieren. Durch die Verwendung von Kameras und bereitgestellter Software ist Pepper in der Lage, Emotionen bei seinen Gesprächspartnern zu erkennen und darauf zu reagieren, obwohl er selbst keine Mimik besitzt. Sicherheitsvorkehrungen wie Abstandssensoren und Stabilisatoren gewährleisten einen sicheren Einsatz von Pepper in verschiedenen Umgebungen. Diese können jedoch bedingt durch den Entwickler deaktiviert werden, um den Roboter in komplexeren oder laborähnlichen Umgebungen zu betreiben.

\section{NAOqi}
\label{sec:NAOqi}

\subsection{Definition}
\label{subsec:NAOqi}
\par NAOqi ist die Bezeichnung für die Hauptsoftware, die auf dem Pepper Roboter ausgeführt wird und ihn intern steuert. Das NAOqi Framework ist das Programmiergerüst, welches zur Programmierung von NAO und Pepper Robotern verwendet wird. Es implementiert alle allgemeinen Anforderungen der Robotik, einschließlich: Parallelität, Ressourcen-Management, Synchronisation und Ereignisse. Dieses Framework ermöglicht eine homogene Kommunikation zwischen verschiedenen Modulen wie etwa die Bewegung, Audio oder Video sowie eine homogene Programmierung und einen homogenen Informationsaustausch. Das Framework ist:
\begin{itemize}
    \item plattformübergreifend, d.h. es ist möglich, damit auf Windows, Linux oder Mac zu entwickeln. Genaueres dazu im \autoref{subsubsec:UnterstützteBetriebssysteme}.
    \item sprachübergreifend, mit einer identischen API für C++ und Python. Weitere Details dazu sind in \autoref{subsubsec:Sprachuebergreifend} aufgeführt.
    \item bereit für Introspektion, was bedeutet, dass das Framework weiß, welche Funktionen in den verschiedenen Modulen verfügbar sind und wo. Für Details diesbezüglich siehe \autoref{subsubsec:Introspektion}.
\end{itemize}

\subsubsection{Sprachübergreifend}
\label{subsubsec:Sprachuebergreifend}
\par Software kann in C++ und Python entwickelt werden. Eine Übersicht über die Sprachen selbst in den Abschnitten \autoref{subsec:Cpp} und \autoref{subsec:Python}. In allen Fällen sind die Programmiermethoden genau die gleichen, alle vorhandenen \acp{API} können unabhängig von den unterstützten Sprachen aufgerufen werden:
\begin{itemize}
    \item Wird ein neues C++-Modul erstellt, können die C++-\ac{API}-Funktionen von überall aus aufgerufen werden,
    \item Sind sie richtig definiert, können auch die \ac{API}-Funktionen eines Python-Moduls von überall aus aufgerufen werden.
\end{itemize}
\par In der Regel werden die Verhaltensweisen in Python und Ihre Dienste in C++ entwickelt.
%TODO{Image of Sprachuebergreifend}

\subsubsection{Introspektion}
\label{subsubsec:Introspektion}
\par Die Introspektion ist die Grundlage der Roboter-\ac{API}, der Fähigkeiten, der Überwachung und der Maßnahmen bei überwachten Funktionen. Der Roboter selbst kennt alle verfügbaren \ac{API}-Funktionen. Wird eine Bibliothek entladen, werden die entsprechenden \ac{API}-Funktionen automatisch ebenfalls entfernt. Eine in einem Modul definierte Funktion kann der \ac{API} mit einem \texttt{BIND\_METHOD} hinzugefügt werden.
\par Wird eine Funktion gebunden, werder automatisch folgende Funktionen ausgeführt:
\begin{itemize}
    \item Funktionsaufruf in C++ und Python, wie in \autoref{subsubsec:Sprachuebergreifend} beschrieben
    \item Erkennen der Funktion, wenn sie gerade ausgeführt wird
    \item Funktion lokal oder aus der Ferne, z.B. von einem Computer oder einem anderen Roboter, ausführen weiter im Detail beschrieben in \autoref{subsubsec:VerteilterBaumUndKommunikation}
    \item Generierung und Aufruf von \texttt{wait}, \texttt{stop}, \texttt{isRunning} in Funktionen
\end{itemize}
\par Die \ac{API} wird im Webbrowser angezeigt wenn auf das Gerät per \ac{URL} oder \ac{IP}-Addresse auf dem Port 9559 zugegriffen wird. In dieser Übersicht, zeigt der Roboter seine Modulliste, Methodenliste, Methodenparameter, Beschreibungen und Beispiele an. Der Browser zeigt auch parallele Methoden an, die überwacht, zum Warten veranlasst und gestoppt werden können.
\par Die Introspektion und derer Implementation im NAOqi-Framework, ist also ein mächtiges Werkzeug, welches es ermöglicht, die Roboter-\ac{API} zu verstehen und zu verwenden aber auch zu überwachenund zu steuern.
%TODO{Image of Introspektion}

\subsubsection{Verteilter Baum und Kommunikation}
\label{subsubsec:VerteilterBaumUndKommunikation}
\par Eine Echtzeitanwendung kann aus einer einzelnen ausführbaren Datei oder einem Baum von mehreren Systemen wie etwa Robotern, Prozessen oder Modulen bestehen. Unabhängig davon sind die Aufrufmethoden immer dieselben. Eine ausführbare Datei kann durch eine Verbindung mit einem anderen Roboter mit \ac{IP}-Adresse und Port verbunden werden, sodass alle \ac{API}-Methoden von anderen ausführbaren Dateien sind auf die gleiche Weise verfügbar sind, genau wie bei einer lokalen Methode. NAOqi trifft dabei selbst die Wahl zwischen schnellem Direktaufruf \ac{LPC} und Fernaufruf \ac{RPC}.
%TODO{Image of Verteilter Baum und Kommunikation}

\subsubsection{Unterstützte Betriebssysteme}
\label{subsubsec:UnterstützteBetriebssysteme}
%TODO{Unterstützte Betriebssysteme}

\subsection{NAOqi Vorgehensweise}
\label{subsec:NAOqiVorgehensweise}
\par Die NAOqi Software, welche auf dem Roboter läuft, ist ein Broker. Wenn dieser startet, lädt er eine Voreinstellungsdatei in den Speicher, in der festgelegt ist, welche Bibliotheken in dieser Konfiguration geladen werden sollen. Jede Bibliothek enthält ein oder mehrere Module, die den Broker benutzen, um ihre Methoden bereitzustellen.
%TODO{Image of NAOqi Vorgehensweise0}
\par Der Broker selbst bietet Nachschlagdienste an, so dass jedes Modul im Baum oder im Netzwerk jede Methode finden kann, die an dem Broker bekannt gegeben wurde.
\par Das Laden von Modulen bildet dann einen Baum von Methoden, die mit Modulen verbunden sind, und von Modulen, die mit dem Broker verbunden sind.
%TODO{Image of NAOqi Vorgehensweise1}

\subsection{Broker}
\label{subsec:Broker}







\section{Robot Operating System}
\label{sec:ROS}

\section{Programming Languages}

\subsection{C++}
\label{subsec:Cpp}

\subsection{Python}
\label{subsec:Python}

<<<<<<< Updated upstream
\section{Virtual Reality}
Unter VR versteht man eine digitale, künstliche Welt.
=======
\subsection{C\#}
C\# (ausgesprochen "C-Sharp") ist eine moderne, objektorientierte Programmiersprache, die von Microsoft im Jahr 2000 als Teil seiner .NET-Initiative entwickelt wurde. Die Sprache wurde unter der Leitung von Anders Hejlsberg entwickelt und ist stark von anderen populären Sprachen wie C, C++ und Java inspiriert. C\# ist für seine Vielseitigkeit, Robustheit und Benutzerfreundlichkeit bekannt und wird in einer Vielzahl von Anwendungen eingesetzt, von Desktop- und Web-Anwendungen bis hin zu mobilen Apps und Spielen, insbesondere in der Entwicklung mit der Unity-Engine.

\subsection{Syntax und Grundstruktur}

Die Syntax von C\# ist einfach und klar, was es für Entwickler leicht macht, die Sprache zu erlernen und zu verwenden.
\subsection{Objektorientierte Programmierung (OOP)}

C\# unterstützt die vier Hauptprinzipien der objektorientierten Programmierung:

1. \textbf{Abstraktion}: Das Verbergen komplexer Implementierungsdetails und das Zeigen nur der notwendigen Eigenschaften eines Objekts.
2. \textbf{Kapselung}: Das Zusammenfassen von Daten und Methoden, die auf diese Daten zugreifen, innerhalb einer Klasse und das Verbergen der Details der Implementierung vor anderen Klassen.
3. \textbf{Vererbung}: Die Fähigkeit einer Klasse, die Eigenschaften und Methoden einer anderen Klasse zu erben, was Code-Wiederverwendung und Hierarchien ermöglicht.
4. \textbf{Polymorphismus}: Die Fähigkeit, eine Methode auf verschiedene Weise zu implementieren oder zu überschreiben, was eine flexible und dynamische Nutzung von Methoden ermöglicht.
\subsection{Vorteile von C\#}

C\# bietet mehrere Vorteile, die es zu einer beliebten Wahl für Entwickler machen:

- \textbf{Einfach zu erlernen}: Die klare Syntax und die umfangreiche Dokumentation machen C\# zu einer anfängerfreundlichen Sprache.
- \textbf{Leistungsfähig und flexibel}: C\# ist leistungsfähig genug für komplexe Anwendungen und flexibel genug für schnelle Entwicklung.
- \textbf{Große Community und Ressourcen}: Die große Entwicklergemeinschaft und die Fülle an Online-Ressourcen, Tutorials und Foren machen es einfach, Unterstützung zu finden und Wissen auszutauschen.
- \textbf{Integrierte Entwicklungsumgebungen (IDEs)}: Tools wie Visual Studio und Visual Studio Code bieten leistungsstarke Entwicklungsumgebungen mit Debugging- und Code-Analyse-Funktionen.
\section{Virtual Reality}
Virtual Reality (VR) hat sich in den letzten Jahrzehnten von einem visionären Konzept zu einer greifbaren Technologie entwickelt, die in zahlreichen Bereichen unseres Lebens Anwendung findet. VR bezeichnet computergenerierte, dreidimensionale Umgebungen, die durch spezielle Hardware wie VR-Brillen und Bewegungssensoren eine immersive Erfahrung ermöglichen. Benutzer können in diese virtuellen Welten eintauchen und mit ihnen interagieren, was das Gefühl vermittelt, tatsächlich dort anwesend zu sein.
\\

\noindent
Die Wurzeln der VR-Technologie reichen bis in die 1960er Jahre zurück. Einer der frühesten Vorläufer moderner VR-Systeme war das Sensorama, das 1962 von Morton Heilig entwickelt wurde. Das Sensorama bot multisensorische Erlebnisse und kann als eines der ersten Systeme betrachtet werden, das den Nutzer vollständig in eine künstliche Umgebung eintauchen ließ \cite{heilig1962}. Ein weiterer bedeutender Meilenstein war das "Sword of Damocles", das erste echte VR-Headset, das 1968 von Ivan Sutherland entwickelt wurde. Es war ein klobiges Gerät, das an der Decke montiert werden musste, und bot eine rudimentäre Form der virtuellen Realität \cite{sutherland1968}.
\\

\noindent
In den 1980er und 1990er Jahren erlebte VR einen weiteren Aufschwung, insbesondere durch die Arbeit von Jaron Lanier, der den Begriff „Virtual Reality“ populär machte und die Firma VPL Research gründete, die einige der ersten kommerziellen VR-Produkte entwickelte \cite{lanier1992}. Trotz dieser Fortschritte blieb VR lange Zeit eine Nischentechnologie, hauptsächlich aufgrund der hohen Kosten und technischen Einschränkungen.
\\

\noindent
Erst in den 2010er Jahren, mit der Einführung moderner, kostengünstigerer VR-Headsets wie der Oculus Rift, entwickelte sich VR zu einer breiter zugänglichen Technologie. Die Oculus Rift, ursprünglich 2012 auf Kickstarter finanziert, revolutionierte den Markt und führte zu einer neuen Welle von Innovationen in der VR-Technologie \cite{luckey2012}. Kurz darauf folgten andere bedeutende Systeme wie die HTC Vive und PlayStation VR, die die VR-Erfahrung weiter verbesserten und breitere Zielgruppen erreichten.
\\

\noindent
Die heutige VR-Technologie zeichnet sich durch hochauflösende Displays, präzises Tracking und eine Vielzahl von Eingabemethoden aus, die eine immersive und interaktive Erfahrung ermöglichen. Meta Quest 3 (ehemals Oculus Quest 3) ist ein Beispiel für ein modernes, eigenständiges VR-Headset, das keine Verbindung zu einem leistungsstarken PC benötigt und dennoch eine beeindruckende Leistung bietet \cite{meta2023}.
\\

\noindent
Die Anwendungsbereiche von VR sind vielfältig und umfassen nicht nur Unterhaltung und Spiele, sondern auch Bildung, medizinische Therapie, Training und Simulationen, Architektur und Design sowie viele andere Felder. Zum Beispiel wird VR in der Medizin zur Behandlung von Phobien, in der Schmerztherapie und in der Rehabilitation eingesetzt \cite{rizzo2017}. In der Ausbildung ermöglicht VR realitätsnahe Trainingsumgebungen, die sicher und kontrolliert sind, was insbesondere in der Luftfahrt und der Medizin von großem Nutzen ist \cite{huang2018}.
\\

\noindent
Die rapide Weiterentwicklung von Hardware und Software sowie die steigende Akzeptanz und Integration von VR in verschiedenen Lebensbereichen zeigen, dass diese Technologie das Potenzial hat, unsere Interaktion mit digitalen Inhalten und unsere Wahrnehmung von Realität grundlegend zu verändern. Die folgenden Abschnitte dieser Arbeit werden die technologischen Grundlagen von VR, verschiedene Anwendungsbereiche sowie die aktuellen Herausforderungen und Zukunftsaussichten der Technologie detailliert untersuchen.
>>>>>>> Stashed changes

\section{Entwicklung für Virtual Reality}
Die Entwicklungsumgebungen für VR sind Softwareplattformen, welche den Entwicklern die Werkzeuge und Funktionen zur Erstellung von Anwendungen für VR-Brillen bieten. Die zwei bekanntesten und am häufigsten verwendeten sind Unity und Unreal Engine. Diese beiden bieten eine umfassende Unterstützung für die VR-Entwicklung. Sie werden in vielen Anwendungen und Spielen und auch bei industriellen Lösungen verwendet. Im folgenden werden beide Entwicklungsplattformen vorgestellt, in Kapitel 3 wird dann erläutert für welche der beiden Plattformen sich für die Entwicklung entschieden wurde.
\subsection{Unity}
Unity ist eine weit verbreitete, benutzerfreundliche Entwicklungsplattform, die sich durch ihre Vielseitigkeit und umfangreiche Toolsets auszeichnet. Unity bietet integrierte Unterstützung für VR und wird häufig aufgrund seiner Benutzerfreundlichkeit und der breiten Community bevorzugt.
\paragraph{Benutzerfreundlichkeit}
Unity hat eine intuitive Benutzeroberfläche, für welche die Plattform auch bekannt ist. Ebenfalls bekannt ist es für die leichte Erlernbarkeit, weshalb es sowohl Einsteigern als auch erfahrenen Entwicklern zusagt \cite{unity2021}.
Unterstützte Plattformen: Unity unterstützt eine Vielzahl von VR-Plattformen, darunter Oculus Rift, HTC Vive, PlayStation VR und verschiedene mobile VR-Headsets wie Google Cardboard und Samsung Gear VR \cite{unity2021}.
\paragraph{Asset Store}
Der Unity Asset Store bietet eine große Auswahl an vorgefertigten Assets, Plugins und Tools, die die Entwicklung von VR-Anwendungen erleichtern und beschleunigen \cite{unityAssetStore2021}.
\paragraph{Scripting}
Unity verwendet C\# als Hauptprogrammiersprache, was Entwicklern ermöglicht, komplexe Interaktionen und Animationen zu erstellen \cite{unity2021}.
\subsection{Unreal Engine}
Unreal Engine wurde von Epic Games entwickelt und ist ebenfalls eine führende Plattform für die VR Entwicklung. Diese Entwicklungsumgebung ist vor allem für ihre leistungsstarke Grafik und Rendering-Fähigkeiten bekannt .
\paragraph{Grafikqualität} Die Grafikqualität und realistischen visuellen Effekte, welche besonders hochwertige Spiele wichtig sind, sind bei Unreal Engine besonders ausgeprägt \cite{epic2021}
\paragraph{Blueprint System} Unreal Engine bietet das Blueprints Visual Scripting System, das es Entwicklern ermöglicht, ohne tiefgreifende Programmierkenntnisse komplexe Interaktionen zu erstellen \cite{epicBlueprints2021}.
\paragraph{Unterstützte Plattformen} Wie Unity unterstützt auch Unreal Engine eine breite Palette von VR-Plattformen, einschließlich Oculus Rift, HTC Vive und PlayStation VR \cite{epic2021}.
\paragraph{Open Source}
Ein großer Vorteil der Unreal Engine ist ihr Open-Source-Charakter, der Entwicklern vollständigen Zugang zum Quellcode der Engine bietet, was tiefergehende Anpassungen und Optimierungen ermöglicht \cite{epic2021}.
\section{Unity-ROS TCP Controller}

\subsection{Einführung}

Der Unity-ROS TCP Controller ist ein leistungsstarkes Tool, das es ermöglicht, Robot Operating System (ROS) mit der Unity-Engine zu verbinden. Diese Integration bietet Entwicklern die Möglichkeit, VR- und AR-Anwendungen zu erstellen, die mit realen Robotern interagieren können, indem sie Daten zwischen ROS und Unity austauschen. Diese Funktionalität ist besonders nützlich in Bereichen wie Robotik, Simulationen und erweiterter Realität, wo die Interaktion zwischen virtuellen und physischen Welten von zentraler Bedeutung ist \cite{ros_tcp_endpoint,unity_ros_tcp_connector}.

\subsection{Grundlagen und Architektur}

Der Unity-ROS TCP Controller funktioniert durch die Verwendung eines TCP-Protokolls zur Kommunikation zwischen ROS und Unity. Dies ermöglicht eine zuverlässige und bidirektionale Datenübertragung, die für Anwendungen erforderlich ist, die auf Echtzeitdaten angewiesen sind. Die grundlegende Architektur besteht aus zwei Hauptkomponenten:

\begin{enumerate}
    \item \textbf{ROS TCP Endpoint}: Dies ist der Server, der auf der ROS-Seite läuft und auf eingehende Verbindungen von Unity wartet. Er empfängt Nachrichten von Unity und sendet Nachrichten an Unity \cite{ros_tcp_endpoint}.
    \item \textbf{Unity TCP Connector}: Dies ist der Client, der in Unity läuft und eine Verbindung zum ROS TCP Endpoint herstellt. Er sendet Nachrichten an ROS und empfängt Nachrichten von ROS \cite{unity_ros_tcp_connector}.
\end{enumerate}

\subsection{Einrichtung und Verwendung}

Die Einrichtung des Unity-ROS TCP Controllers umfasst mehrere Schritte, sowohl auf der ROS- als auch auf der Unity-Seite. Hier ist eine allgemeine Anleitung zur Einrichtung:

\subsubsection{ROS-Seite}

\begin{itemize}
    \item Installiere die erforderlichen ROS-Pakete, z.B. \texttt{ros\_tcp\_endpoint} \cite{ros_tcp_endpoint}.
    \item Starte den ROS TCP Endpoint:
    \begin{verbatim}
roslaunch ros_tcp_endpoint endpoint.launch
    \end{verbatim}
\end{itemize}

\subsubsection{Unity-Seite}

\begin{itemize}
    \item Importiere das ROS-TCP-Connector-Paket in dein Unity-Projekt \cite{unity_ros_tcp_connector}.
    \item Füge das \texttt{RosConnector}-Skript zu einem GameObject in deiner Szene hinzu und konfiguriere die IP-Adresse und den Port des ROS-TCP-Endpunkts.
    \item Erstelle Nachrichten-Typen in Unity, die mit den ROS-Nachrichten übereinstimmen, die du senden und empfangen möchtest.
    \item Verwende das \texttt{RosConnector}-Skript, um Nachrichten an ROS zu senden und Nachrichten von ROS zu empfangen.
\end{itemize}

\subsection{Beispiel für Unity-Skript}

Hier ist ein Beispiel für ein Unity-Skript, das den Unity-ROS TCP Controller verwendet, um Positionsdaten an ROS zu senden:

\begin{verbatim}
using UnityEngine;
using Unity.Robotics.ROSTCPConnector;
using RosMessageTypes.Geometry;

public class PositionPublisher : MonoBehaviour
{
    ROSConnection ros;
    public string topicName = "/unity/position";
    public float publishRate = 0.5f;

    private float timeElapsed;

    void Start()
    {
        ros = ROSConnection.instance;
        ros.RegisterPublisher<PointMsg>(topicName);
    }

    void Update()
    {
        timeElapsed += Time.deltaTime;

        if (timeElapsed > publishRate)
        {
            PointMsg positionMessage = new PointMsg(
                transform.position.x,
                transform.position.y,
                transform.position.z
            );

            ros.Publish(topicName, positionMessage);

            timeElapsed = 0;
        }
    }
}
\end{verbatim}

In diesem Beispiel wird die Position eines GameObjects in Unity periodisch an ein ROS-Thema gesendet. Das Skript:

\begin{itemize}
    \item Registriert einen Publisher für das angegebene ROS-Thema.
    \item Sendet die Position des GameObjects als \texttt{PointMsg}-Nachricht an ROS \cite{unity_ros_tcp_connector}.
\end{itemize}

\subsection{Vorteile und Anwendungsbereiche}

Der Unity-ROS TCP Controller bietet zahlreiche Vorteile:

\begin{itemize}
    \item \textbf{Echtzeit-Interaktion}: Ermöglicht die Echtzeit-Interaktion zwischen virtuellen und physischen Robotern.
    \item \textbf{Flexibilität}: Unterstützt verschiedene ROS-Nachrichtentypen und ermöglicht die Anpassung an spezifische Anwendungsanforderungen.
    \item \textbf{Erweiterbarkeit}: Kann in verschiedene VR/AR-Projekte integriert werden, um erweiterte Robotik-Simulationen und Visualisierungen zu erstellen.
\end{itemize}

Anwendungsbereiche umfassen:

\begin{itemize}
    \item \textbf{Robotik-Forschung}: Simulation und Testen von Robotern in virtuellen Umgebungen.
    \item \textbf{Bildung}: Interaktive Lernumgebungen für die Robotik-Ausbildung.
    \item \textbf{Industrie}: Visualisierung und Steuerung von Industrierobotern in einer virtuellen Umgebung.
\end{itemize}

\subsection{Fazit}

Der Unity-ROS TCP Controller ist ein mächtiges Werkzeug, das die Lücke zwischen virtuellen Simulationen in Unity und realen Robotersystemen, die auf ROS basieren, schließt. Durch die Nutzung von TCP für die Kommunikation bietet es eine robuste und flexible Lösung für eine Vielzahl von Anwendungen in der Robotik und darüber hinaus.
\section{Unreal Engine ROS Integration}

\subsection{Einführung}

Die Unreal Engine ROS Integration ermöglicht die Verbindung und Interaktion zwischen dem Robot Operating System (ROS) und der Unreal Engine. Diese Integration bietet ähnliche Funktionen wie der Unity-ROS TCP Controller und ist nützlich für die Erstellung von Simulationen, Visualisierungen und interaktiven Anwendungen, die mit physischen Robotern kommunizieren \cite{ros_integration_github}.

\subsection{Grundlagen und Architektur}

Das ROSIntegration Plugin für die Unreal Engine besteht aus mehreren Komponenten, die zusammenarbeiten, um eine nahtlose Kommunikation zwischen ROS und der Unreal Engine zu gewährleisten. Die grundlegende Architektur umfasst:

\begin{enumerate}
    \item \textbf{ROS Nodes}: Diese werden in ROS definiert und dienen zur Kommunikation und Datenverarbeitung.
    \item \textbf{Unreal Engine ROS Nodes}: Diese Nodes werden in der Unreal Engine erstellt und fungieren als Schnittstelle zu den ROS Nodes.
\end{enumerate}

\subsection{Einrichtung und Verwendung}

Die Einrichtung des ROSIntegration Plugins in der Unreal Engine umfasst mehrere Schritte:

\subsubsection{Installation des Plugins}

\begin{itemize}
    \item Lade das ROSIntegration Plugin von der offiziellen GitHub-Seite herunter: \url{https://github.com/code-iai/ROSIntegration}.
    \item Füge das Plugin zu deinem Unreal Engine Projekt hinzu und aktiviere es in den Projekteinstellungen.
\end{itemize}

\subsubsection{Konfiguration des Plugins}

\begin{itemize}
    \item Konfiguriere die IP-Adresse und den Port des ROS Masters, mit dem die Unreal Engine kommunizieren soll.
    \item Stelle sicher, dass das ROS Master läuft und erreichbar ist.
\end{itemize}

\subsubsection{Erstellen von ROS-Komponenten in Unreal}

\begin{itemize}
    \item Erstelle ROS-spezifische Komponenten wie Publisher und Subscriber in der Unreal Engine, um Daten zu senden und zu empfangen.
\end{itemize}

\subsection{Beispiel für Unreal Engine Skript}

Hier ist ein Beispiel für die Verwendung des ROSIntegration Plugins in der Unreal Engine, um Positionsdaten zu veröffentlichen:

\begin{verbatim}
#include "ROSIntegrationGameMode.h"
#include "ROSIntegration/Public/ROSIntegrationGameInstance.h"
#include "ROSIntegration/Public/Publisher.h"
#include "ROSIntegration/Public/std_msgs/String.h"

void AROSIntegrationGameMode::BeginPlay()
{
    Super::BeginPlay();

    UROSIntegrationGameInstance* ROSInst = 
        Cast<UROSIntegrationGameInstance>(GetGameInstance());
    if (ROSInst)
    {
        ROSInst->Init();

        TSharedPtr<ROSPublisher> Publisher = 
            MakeShareable(new ROSPublisher(ROSInst->ROSIntegrationCore, 
                                           TEXT("/unity/position"), 
                                           TEXT("geometry_msgs/Point")));
        Publisher->Advertise();
        
        // Publish a message
        FROSTime now = ROSInst->ROSIntegrationCore->ROSTimeNow();
        TSharedPtr<ROSMessages::geometry_msgs::Point> PointMessage = 
            MakeShareable(new ROSMessages::geometry_msgs::Point());
        PointMessage->x = GetActorLocation().X;
        PointMessage->y = GetActorLocation().Y;
        PointMessage->z = GetActorLocation().Z;
        Publisher->Publish(PointMessage);
    }
}
\end{verbatim}

In diesem Beispiel wird die Position eines Akteurs (Actors) in der Unreal Engine an ein ROS-Thema gesendet. Das Skript:

\begin{itemize}
    \item Initialisiert das ROSIntegration Plugin.
    \item Erstellt einen Publisher für das angegebene ROS-Thema.
    \item Sendet die Position des Akteurs als \texttt{geometry\_msgs/Point} Nachricht an ROS \cite{ros_integration_github}.
\end{itemize}

\subsection{Vorteile und Anwendungsbereiche}

Die Integration von ROS in die Unreal Engine bietet zahlreiche Vorteile:

\begin{itemize}
    \item \textbf{Echtzeit-Interaktion}: Ermöglicht die Echtzeit-Interaktion zwischen virtuellen und physischen Robotern.
    \item \textbf{Visuelle Genauigkeit}: Die Unreal Engine bietet hochwertige Grafiken und realistische Visualisierungen, die für Simulationen und Präsentationen nützlich sind.
    \item \textbf{Flexibilität}: Unterstützt verschiedene ROS-Nachrichtentypen und ermöglicht die Anpassung an spezifische Anwendungsanforderungen.
\end{itemize}

Anwendungsbereiche umfassen:

\begin{itemize}
    \item \textbf{Robotik-Forschung}: Simulation und Testen von Robotern in virtuellen Umgebungen.
    \item \textbf{Bildung}: Interaktive Lernumgebungen für die Robotik-Ausbildung.
    \item \textbf{Industrie}: Visualisierung und Steuerung von Industrierobotern in einer virtuellen Umgebung.
\end{itemize}

\subsection{Fazit}

Die Unreal Engine ROS Integration ist ein mächtiges Werkzeug, das die Lücke zwischen virtuellen Simulationen in der Unreal Engine und realen Robotersystemen, die auf ROS basieren, schließt. Durch die Nutzung dieses Plugins können Entwickler hochwertige Simulationen und Anwendungen erstellen, die eine nahtlose Integration von ROS-Daten in die Unreal Engine ermöglichen.
